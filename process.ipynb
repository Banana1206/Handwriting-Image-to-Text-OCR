{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download and import the dataset of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/l2ul3upj7dkv4ou/synthetic-data.zip\n",
    "!unzip -qq synthetic-data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import the packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_snippets import *\n",
    "from torchsummary import summary\n",
    "import editdistance\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "fname2label = lambda fname: stem(fname).split('@')[0]\n",
    "images = Glob('synthetic-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[255, 255, 255, ..., 255, 255, 255],\n",
       "       [255, 255, 255, ..., 255, 255, 255],\n",
       "       [255, 255, 255, ..., 255, 255, 255],\n",
       "       ...,\n",
       "       [255, 255, 255, ..., 255, 255, 255],\n",
       "       [255, 255, 255, ..., 255, 255, 255],\n",
       "       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imread(str(images[0]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'about'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname2label(images[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the vocabulary of characters `(vocab)`, the batch size `(B)`, the time\n",
    "steps of the RNN `(T)`, the length of the vocabulary `(V)`, the height `(H)`, and\n",
    "the width `(W)` of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab='QWERTYUIOPASDFGHJKLZXCVBNMqwertyuiopasdfghjklzxcvbnm'\n",
    "B,T,V = 64, 32, len(vocab)\n",
    "H,W = 32, 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the OCRDataset dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, items, vocab=vocab, preprocess_shape=(H,W), timesteps=T):\n",
    "        super().__init__()\n",
    "        self.items = items\n",
    "        self.charList = {ix+1:ch for ix,ch in enumerate(vocab)}\n",
    "        self.charList.update({0: '`'})\n",
    "        self.invCharList = {v:k for k,v in self.charList.items()}\n",
    "        self.ts = timesteps\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    def sample(self):\n",
    "        return self[randint(len(self))]\n",
    "    def __getitem__(self, ix):\n",
    "        item = self.items[ix]\n",
    "        image = cv2.imread(str(item), 0)\n",
    "        label = fname2label(item)\n",
    "        return image, label\n",
    "    def collate_fn(self, batch):\n",
    "        images, labels, label_lengths, label_vectors, input_lengths = [], [], [], [], []\n",
    "        for image, label in batch:\n",
    "            images.append(torch.Tensor(self.preprocess(image))[None,None])\n",
    "            label_lengths.append(len(label))\n",
    "            labels.append(label)\n",
    "            label_vectors.append(self.str2vec(label))\n",
    "            input_lengths.append(self.ts)\n",
    "        images = torch.cat(images).float().to(device)\n",
    "        label_lengths = torch.Tensor(label_lengths).long().to(device)\n",
    "        label_vectors = torch.Tensor(label_vectors).long().to(device)\n",
    "        input_lengths = torch.Tensor(input_lengths).long().to(device)\n",
    "        return images, label_vectors, label_lengths, input_lengths, labels\n",
    "    def str2vec(self, string, pad=True):\n",
    "        string = ''.join([s for s in string if s in self.invCharList])\n",
    "        val = list(map(lambda x: self.invCharList[x], string)) \n",
    "        if pad:\n",
    "            while len(val) < self.ts:\n",
    "                val.append(0)\n",
    "        return val\n",
    "    def preprocess(self, img, shape=(32,128)):\n",
    "        target = np.ones(shape)*255\n",
    "        try:\n",
    "            H, W = shape\n",
    "            h, w = img.shape\n",
    "            fx = H/h\n",
    "            fy = W/w\n",
    "            f = min(fx, fy)\n",
    "            _h = int(h*f)\n",
    "            _w = int(w*f)\n",
    "            _img = cv2.resize(img, (_w,_h))\n",
    "            target[:_h,:_w] = _img\n",
    "        except:\n",
    "            ...\n",
    "        return (255-target)/255\n",
    "    def decoder_chars(self, pred):\n",
    "        decoded = \"\"\n",
    "        last = \"\"\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "        for i in range(len(pred)):\n",
    "            k = np.argmax(pred[i])\n",
    "            if k > 0 and self.charList[k] != last:\n",
    "                last = self.charList[k]\n",
    "                decoded = decoded + last\n",
    "            elif k > 0 and self.charList[k] == last:\n",
    "                continue\n",
    "            else:\n",
    "                last = \"\"\n",
    "        return decoded.replace(\" \",\" \")\n",
    "    def wer(self, preds, labels):\n",
    "        c = 0\n",
    "        for p, l in zip(preds, labels):\n",
    "            c += p.lower().strip() != l.lower().strip()\n",
    "        return round(c/len(preds), 4)\n",
    "    def cer(self, preds, labels):\n",
    "        c, d = [], []\n",
    "        for p, l in zip(preds, labels):\n",
    "            c.append(editdistance.eval(p, l) / len(l))\n",
    "        return round(np.mean(c), 4)\n",
    "    def evaluate(self, model, ims, labels, lower=False):\n",
    "        model.eval()\n",
    "        preds = model(ims).permute(1,0,2) # B, T, V+1\n",
    "        preds = [self.decoder_chars(pred) for pred in preds]\n",
    "        return {'char-error-rate': self.cer(preds, labels),\n",
    "                'word-error-rate': self.wer(preds, labels),\n",
    "                'char-accuracy' : 1 - self.cer(preds, labels),\n",
    "                'word-accuracy' : 1 - self.wer(preds, labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Specify the training and validation datasets and the dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trn_items, val_items = train_test_split(Glob('synthetic-data'), test_size=0.2, random_state=22)\n",
    "trn_ds = OCRDataset(trn_items)\n",
    "val_ds = OCRDataset(val_items)\n",
    "\n",
    "trn_dl = DataLoader(trn_ds, batch_size=B, collate_fn=trn_ds.collate_fn, drop_last=True, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=B, collate_fn=val_ds.collate_fn, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255],\n",
       "        [255, 255, 255, ..., 255, 255, 255]], dtype=uint8),\n",
       " 'risk')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds.__getitem__(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Build the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_snippets import Reshape, Permute\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, ni, no, ks=3, st=1, padding=1, pool=2, drop=0.2):\n",
    "        super().__init__()\n",
    "        self.ks = ks\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(ni, no, kernel_size=ks, stride=st, padding=padding),\n",
    "            nn.BatchNorm2d(no, momentum=0.3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(pool),\n",
    "            nn.Dropout2d(drop)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ocr(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            BasicBlock( 1, 128),\n",
    "            BasicBlock(128, 128),\n",
    "            BasicBlock(128, 256, pool=(4,2)),\n",
    "            Reshape(-1, 256, 32),\n",
    "            Permute(2, 0, 1) # T, B, D\n",
    "        )\n",
    "        self.rnn = nn.Sequential(\n",
    "            nn.LSTM(256, 256, num_layers=2, dropout=0.2, bidirectional=True),\n",
    "        )\n",
    "        self.classification = nn.Sequential(\n",
    "            nn.Linear(512, vocab+1),\n",
    "            nn.LogSoftmax(-1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x, lstm_states = self.rnn(x)\n",
    "        y = self.classification(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CTC loss function:\n",
    "\n",
    "def ctc(log_probs, target, input_lengths, target_lengths, blank=0):\n",
    "    loss = nn.CTCLoss(blank=blank, zero_infinity=True)\n",
    "    ctc_loss = loss(log_probs, target, input_lengths, target_lengths)\n",
    "    return ctc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 1, 256]              --\n",
      "|    └─BasicBlock: 2-1                   [-1, 128, 16, 64]         --\n",
      "|    |    └─Sequential: 3-1              [-1, 128, 16, 64]         1,536\n",
      "|    └─BasicBlock: 2-2                   [-1, 128, 8, 32]          --\n",
      "|    |    └─Sequential: 3-2              [-1, 128, 8, 32]          147,840\n",
      "|    └─BasicBlock: 2-3                   [-1, 256, 2, 16]          --\n",
      "|    |    └─Sequential: 3-3              [-1, 256, 2, 16]          295,680\n",
      "|    └─Reshape: 2-4                      [-1, 256, 32]             --\n",
      "|    └─Permute: 2-5                      [-1, 1, 256]              --\n",
      "├─Sequential: 1-2                        [-1, 1, 512]              --\n",
      "|    └─LSTM: 2-6                         [-1, 1, 512]              2,629,632\n",
      "├─Sequential: 1-3                        [-1, 1, 53]               --\n",
      "|    └─Linear: 2-7                       [-1, 1, 53]               27,189\n",
      "|    └─LogSoftmax: 2-8                   [-1, 1, 53]               --\n",
      "==========================================================================================\n",
      "Total params: 3,101,877\n",
      "Trainable params: 3,101,877\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 237.84\n",
      "==========================================================================================\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 11.00\n",
      "Params size (MB): 11.83\n",
      "Estimated Total Size (MB): 22.85\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 1, 256]              --\n",
       "|    └─BasicBlock: 2-1                   [-1, 128, 16, 64]         --\n",
       "|    |    └─Sequential: 3-1              [-1, 128, 16, 64]         1,536\n",
       "|    └─BasicBlock: 2-2                   [-1, 128, 8, 32]          --\n",
       "|    |    └─Sequential: 3-2              [-1, 128, 8, 32]          147,840\n",
       "|    └─BasicBlock: 2-3                   [-1, 256, 2, 16]          --\n",
       "|    |    └─Sequential: 3-3              [-1, 256, 2, 16]          295,680\n",
       "|    └─Reshape: 2-4                      [-1, 256, 32]             --\n",
       "|    └─Permute: 2-5                      [-1, 1, 256]              --\n",
       "├─Sequential: 1-2                        [-1, 1, 512]              --\n",
       "|    └─LSTM: 2-6                         [-1, 1, 512]              2,629,632\n",
       "├─Sequential: 1-3                        [-1, 1, 53]               --\n",
       "|    └─Linear: 2-7                       [-1, 1, 53]               27,189\n",
       "|    └─LogSoftmax: 2-8                   [-1, 1, 53]               --\n",
       "==========================================================================================\n",
       "Total params: 3,101,877\n",
       "Trainable params: 3,101,877\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 237.84\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 11.00\n",
       "Params size (MB): 11.83\n",
       "Estimated Total Size (MB): 22.85\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Ocr(len(vocab)).to(device)\n",
    "summary(model, torch.zeros((1,1,32,128)).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(data, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    imgs, targets, label_lens, input_lens, labels = data\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(imgs)\n",
    "    loss = criterion(preds, targets, input_lens, label_lens)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    results = trn_ds.evaluate(model, imgs.to(device), labels)\n",
    "    return loss, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_batch(data, model, criterion):\n",
    "    model.eval()\n",
    "    imgs, targets, label_lens, input_lens, labels = data\n",
    "    preds = model(imgs)\n",
    "    loss = criterion(preds, targets, input_lens, label_lens)\n",
    "    return loss, val_ds.evaluate(model, imgs.to(device), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ocr(len(vocab)).to(device)\n",
    "criterion = ctc\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-3)\n",
    "\n",
    "n_epochs = 1\n",
    "log = Report(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1.000  val_char_acc: 0.088  trn_char_acc: 0.023  trn_word_acc: 0.000  trn_loss: 3.318  val_loss: 3.040  val_word_acc: 0.000  (1475.50s - 0.00s remaining)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pred: `bo` :: Truth: `family`\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Pred: `bo` :: Truth: `family`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pred: `bo` :: Truth: `PM`\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Pred: `bo` :: Truth: `PM`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pred: `po` :: Truth: `without`\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Pred: `po` :: Truth: `without`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pred: `bo` :: Truth: `from`\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Pred: `bo` :: Truth: `from`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pred: `po` :: Truth: `production`\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Pred: `po` :: Truth: `production`\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for ep in range( n_epochs):\n",
    "    # if ep in lr_schedule: optimizer = AdamW(ocr.parameters(), lr=lr_schedule[ep])\n",
    "    N = len(trn_dl)\n",
    "    for ix, data in enumerate(trn_dl):\n",
    "        pos = ep + (ix+1)/N\n",
    "        loss, results = train_batch(data, model, optimizer, criterion)\n",
    "        # scheduler.step()\n",
    "        ca, wa = results['char-accuracy'], results['word-accuracy']\n",
    "        log.record(pos=pos, trn_loss=loss, trn_char_acc=ca, trn_word_acc=wa, end='\\r')\n",
    "    val_results = []\n",
    "    N = len(val_dl)\n",
    "    for ix, data in enumerate(val_dl):\n",
    "        pos = ep + (ix+1)/N\n",
    "        loss, results = validate_batch(data, model, criterion)\n",
    "        ca, wa = results['char-accuracy'], results['word-accuracy']\n",
    "        log.record(pos=pos, val_loss=loss, val_char_acc=ca, val_word_acc=wa, end='\\r')\n",
    "\n",
    "    log.report_avgs(ep+1)\n",
    "    print()\n",
    "    for jx in range(5):\n",
    "        img, label = val_ds.sample()\n",
    "        _img = torch.Tensor(val_ds.preprocess(img)[None,None]).to(device)\n",
    "        pred = model(_img)[:,0,:]\n",
    "        pred = trn_ds.decoder_chars(pred)\n",
    "        print(f'Pred: `{pred}` :: Truth: `{label}`')\n",
    "    print()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./result/model1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Ocr(len(vocab)).to(device)\n",
    "model2.load_state_dict(torch.load(\"./result/model1.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
